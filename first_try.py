# -*- coding: utf-8 -*-
"""first_try.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aqL_cz0LT37UoxcfUnZ8g6MPiL8Lpkef
"""

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.offline as py
import plotly.graph_objs as go
import plotly.tools as tls
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv('/data.csv')

# Labels that need to be removed from posts
lbl_rmv=list(df['type'].unique())
lbl_rmv = [item.lower() for item in lbl_rmv]

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = stopwords.words('english')

from nltk.stem.porter import PorterStemmer

for i in range(0,8675) :  
    df['posts'][i] = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', df['posts'][i])
    df['posts'][i] = re.sub("[^a-zA-Z]", " ", df['posts'][i])
    df['posts'][i] = re.sub(' +', ' ', df['posts'][i]).lower()
    for j in range(0,16):
        df['posts'][i]=re.sub(lbl_rmv[j], ' ', df['posts'][i])
        
df['posts'] = df['posts'].str.strip()

def pre_process(post):
    posts = re.sub('\s+', ' ', post)
    posts = posts.lower()
    posts = posts.split()
    posts = [word for word in posts if not word in set(stopwords.words('english'))]
    ps = PorterStemmer()
    posts = [ps.stem(word) for word in posts]
    posts = ' '.join(posts)
    return posts
    
corpus = df["posts"].apply(pre_process)

df.head()

corpus

type(corpus)

df_new = corpus.to_frame()

df_new['labels']=df['type']
df_new.head()

df_new.info()

map1 = {"I": 0, "E": 1}
map2 = {"N": 0, "S": 1}
map3 = {"T": 0, "F": 1}
map4 = {"J": 0, "P": 1}
df_new['I-E'] = df_new['labels'].astype(str).str[0]
df_new['I-E'] = df_new['I-E'].map(map1)
df_new['N-S'] = df_new['labels'].astype(str).str[1]
df_new['N-S'] = df_new['N-S'].map(map2)
df_new['T-F'] = df_new['labels'].astype(str).str[2]
df_new['T-F'] = df_new['T-F'].map(map3)
df_new['J-P'] = df_new['labels'].astype(str).str[3]
df_new['J-P'] = df_new['J-P'].map(map4)

df_new.head()

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 2000)
features = cv.fit_transform(df_new['posts']).toarray()
IE = df_new.iloc[:, 2].values
NS = df_new.iloc[:, 3].values
TF = df_new.iloc[:, 4].values
JP = df_new.iloc[:, 5].values

from sklearn.model_selection import train_test_split
features_train, features_test, IE_train, IE_test, NS_train, NS_test, TF_train, TF_test, JP_train, JP_test = train_test_split(features, IE,NS,TF,JP, test_size = 0.20, random_state = 0)

from xgboost import XGBClassifier

# fit model on training data
IEB = XGBClassifier()
IEB.fit(features_train, IE_train)
ieb_train=IEB.score(features_train,IE_train)
ieb_test=IEB.score(features_test,IE_test)

NSB = XGBClassifier()
NSB.fit(features_train, NS_train)
nsb_train=NSB.score(features_train,NS_train)
nsb_test=NSB.score(features_test,NS_test)


TFB = XGBClassifier()
TFB.fit(features_train, TF_train)
tfb_train=TFB.score(features_train,TF_train)
tfb_test=TFB.score(features_test,TF_test)


JPB = XGBClassifier()
JPB.fit(features_train, JP_train)
jpb_train=JPB.score(features_train,JP_train)
jpb_test=JPB.score(features_test,JP_test)

print('I-E train score is :',ieb_train)
print('I-E test score is :',ieb_test)
print('N-S train score is :',nsb_train)
print('N-S test score is :',nsb_test)
print('T-F train score is :',tfb_train)
print('T-F test score is :',tfb_test)
print('J-P train score is :',jpb_train)
print('J-P test score is :',jpb_test)

def convert(post):
  # print("1="+post)

  post = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', post)
  posts = re.sub("[^a-zA-Z]", " ", post)
  post = re.sub(' +', ' ', post).lower()
  # print("2="+post)
  for j in range(0,16):
    post=re.sub(lbl_rmv[j], ' ', post)
        
  post = post.strip()
  # print("3="+post)

  post = re.sub('\s+', ' ', post)
  post = post.lower()
  post = post.split()
  # print("1="+post)
  post = [word for word in post if not word in set(stopwords.words('english'))]
  ps = PorterStemmer()
  post = [ps.stem(word) for word in post]
  post = ' '.join(post)

  return post

df_new.iloc[6:12,:]

post = df['posts'][8]

post = convert(post)
post = cv.transform([post]).toarray()

post

IEB.predict(post)

JPB.predict(post)

TFB.predict(post)

NSB.predict(post)

